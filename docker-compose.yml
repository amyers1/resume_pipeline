services:
    # PostgreSQL Database (New)
    db:
        image: postgres
        container_name: resume-postgres
        environment:
            - POSTGRES_USER=${POSTGRES_USER:-postgres}
            - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
            - POSTGRES_DB=${POSTGRES_DB:-resume_pipeline}
        ports:
            - "5432:5432" # Expose to host for local inspection tools
        volumes:
            - pg_data:/var/lib/postgresql
        healthcheck:
            test: ["CMD-SHELL", "pg_isready -U postgres"]
            interval: 10s
            timeout: 5s
            retries: 5
        restart: unless-stopped
        networks:
            - default

    # Redis Cache Server
    redis:
        image: redis:7-alpine
        container_name: resume-redis
        ports:
            - "6379:6379"
        volumes:
            - ./data/redis:/data
        command: redis-server --appendonly yes
        healthcheck:
            test: ["CMD", "redis-cli", "ping"]
            interval: 10s
            timeout: 3s
            retries: 3
        restart: unless-stopped
        networks:
            - default

    # API Service
    api:
        build:
            context: ./backend
            dockerfile: Dockerfile
        container_name: resume-pipeline-api
        ports:
            - "8000:8000"
        env_file:
            - .env
        environment:
            # Ensure DB URL uses the service name 'db'
            - DATABASE_URL=postgresql+psycopg://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@db:5432/${POSTGRES_DB:-resume_pipeline}
        volumes:
            # Map the entire backend directory for hot-reloading
            - ./backend:/app
            # Persist output outside the container if needed
            - ./backend/output:/app/output
            # For one time migration of legacy data
            - ./backend/jobs:/app/jobs
        command: uvicorn api:app --host 0.0.0.0 --port 8000 --reload
        depends_on:
            redis:
                condition: service_healthy
            db:
                condition: service_healthy
        restart: unless-stopped
        networks:
            - default
            - proxy
        labels:
            - "traefik.enable=true"
            - "traefik.http.routers.resume_api.rule=Host(`api.resume-pipeline.myerslab.me`)"
            - "traefik.http.routers.resume_api.entrypoints=websecure"
            - "traefik.http.routers.resume_api.tls.certresolver=cloudflare"
            - "traefik.http.services.resume_api.loadbalancer.server.port=8000"
            - "traefik.docker.network=proxy"

    # Worker Service
    worker:
        build:
            context: ./backend
            dockerfile: Dockerfile
        env_file:
            - .env
        environment:
            - DATABASE_URL=postgresql+psycopg://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@db:5432/${POSTGRES_DB:-resume_pipeline}
        volumes:
            - ./backend:/app
            - ./backend/output:/app/output
        # Updated command to use the new filename
        command: python worker.py
        depends_on:
            redis:
                condition: service_healthy
            db:
                condition: service_healthy
        deploy:
            replicas: 2
        restart: unless-stopped
        networks:
            - default

    # Frontend
    frontend:
        build: ./frontend
        container_name: resume-pipeline-frontend
        ports:
            - "3000:80"
        depends_on:
            - api
        environment:
            - VITE_API_URL=${VITE_API_URL}
        restart: unless-stopped
        networks:
            - default
            - proxy
        labels:
            - "traefik.enable=true"
            - "traefik.http.routers.resume_pipeline.rule=Host(`resume-pipeline.myerslab.me`)"
            - "traefik.http.routers.resume_pipeline.entrypoints=websecure"
            - "traefik.http.routers.resume_pipeline.tls.certresolver=cloudflare"
            - "traefik.http.services.resume_pipeline.loadbalancer.server.port=80"
            - "traefik.docker.network=proxy"

volumes:
    pg_data:

networks:
    default:
        name: resume-pipeline-network
    proxy:
        external: true
